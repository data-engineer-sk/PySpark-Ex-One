{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253d67ca",
   "metadata": {},
   "source": [
    "# Lesson Five : Introduce Aggregate and GroupBy in Pyspark\n",
    "### by Samuel Ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bffb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Use agg()\n",
    "# - GroupBy\n",
    "# - sorted()\n",
    "# - collect() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afe941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570c4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (spark is a variable)\n",
    "# for operation\n",
    "spark=SparkSession.builder.appName('GroupAndAggregate').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c83e2e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.85:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GroupAndAggregate</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1163c3f70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a1294d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test5.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2417c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Major: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed15e20a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------------+------+\n",
      "|            Name|Experience|             Major|Salary|\n",
      "+----------------+----------+------------------+------+\n",
      "|     Edward Wong|        10|  Machine Learning| 43000|\n",
      "|         Tony Fu|         8|  Machine Learning| 35000|\n",
      "|        Megan Ho|         4|      Data Science| 23000|\n",
      "|     Pauline Yau|         3|    Virual Reality| 20000|\n",
      "|     Candy Leung|         7|    Microprocessor| 30000|\n",
      "|  Summer Jeffson|         5|Wearable Computing| 26000|\n",
      "|Combell Kawaskey|         4|      Data Science| 24000|\n",
      "|     Nashu Kwong|         8|      Data Science| 36000|\n",
      "|     Suki Takayi|         6|Wearable Computing| 27000|\n",
      "|       Sudhanshu|        12|  Machine Learning| 55000|\n",
      "|     Wellon Mesk|         9|    Virual Reality| 39500|\n",
      "|       Polly Mak|         4|      Data Science| 23500|\n",
      "|     Kevin Leung|         7|      Data Science| 30000|\n",
      "|   Patrick Tsang|         7|  Machine Learning| 32000|\n",
      "|     Thomas Tsui|         5|      Data Science| 29000|\n",
      "|      Peter Paul|         5|    Virual Reality| 30000|\n",
      "|      Su Bhanshu|        12|  Machine Learning| 75000|\n",
      "|     Subha Umash|        10|Wearable Computing| 46000|\n",
      "|    Donna Summer|         4|      Data Science| 24000|\n",
      "|     Alton Pansy|        10|      Data Science| 58000|\n",
      "+----------------+----------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd157518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  avg(Experience)|\n",
      "+-----------------+\n",
      "|6.809523809523809|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|      avg(Salary)|\n",
      "+-----------------+\n",
      "|34904.76190476191|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggrerate function to calculate \n",
    "# the mean and sum of Salary and Experience\n",
    "df_pyspark.agg({'Experience':'mean'}).show()\n",
    "df_pyspark.agg({'Salary':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4966d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+\n",
      "|             Major|  avg(Experience)|       avg(Salary)|\n",
      "+------------------+-----------------+------------------+\n",
      "|  Machine Learning|              9.8|           48000.0|\n",
      "|    Virual Reality|5.666666666666667|29833.333333333332|\n",
      "|    Microprocessor|              7.0|           30000.0|\n",
      "|      Data Science|             5.75|           30937.5|\n",
      "|Wearable Computing|              6.0|           31500.0|\n",
      "+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use groupBy function to calculate the average (Year of Experience) &\n",
    "# average (Salary)\n",
    "df_pyspark.groupBy('Major').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bf0de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|Experience|avg(Experience)|       avg(Salary)|\n",
      "+----------+---------------+------------------+\n",
      "|        12|           12.0|           65000.0|\n",
      "|         6|            6.0|           27000.0|\n",
      "|         3|            3.0|           23500.0|\n",
      "|         5|            5.0|28333.333333333332|\n",
      "|         9|            9.0|           39500.0|\n",
      "|         4|            4.0|           23625.0|\n",
      "|         8|            8.0|           35500.0|\n",
      "|         7|            7.0|30666.666666666668|\n",
      "|        10|           10.0|           49000.0|\n",
      "+----------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean experience and average salary which grouped by 'Experience' field\n",
    "df_pyspark.groupBy('Experience').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d359af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+\n",
      "|             Major|Experience|count|\n",
      "+------------------+----------+-----+\n",
      "|Wearable Computing|         6|    1|\n",
      "|    Microprocessor|         7|    1|\n",
      "|Wearable Computing|        10|    1|\n",
      "|      Data Science|        10|    1|\n",
      "|  Machine Learning|         7|    1|\n",
      "|      Data Science|         4|    4|\n",
      "|  Machine Learning|        12|    2|\n",
      "|    Virual Reality|         3|    1|\n",
      "|      Data Science|         8|    1|\n",
      "|      Data Science|         5|    1|\n",
      "|  Machine Learning|        10|    1|\n",
      "|    Virual Reality|         9|    1|\n",
      "|Wearable Computing|         5|    1|\n",
      "|      Data Science|         7|    1|\n",
      "|    Virual Reality|         5|    1|\n",
      "|Wearable Computing|         3|    1|\n",
      "|  Machine Learning|         8|    1|\n",
      "+------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(['Major', 'Experience']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702d5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+---------------+-----------+\n",
      "|             Major|Experience|avg(Experience)|avg(Salary)|\n",
      "+------------------+----------+---------------+-----------+\n",
      "|Wearable Computing|         6|            6.0|    27000.0|\n",
      "|    Microprocessor|         7|            7.0|    30000.0|\n",
      "|Wearable Computing|        10|           10.0|    46000.0|\n",
      "|      Data Science|        10|           10.0|    58000.0|\n",
      "|  Machine Learning|         7|            7.0|    32000.0|\n",
      "|      Data Science|         4|            4.0|    23625.0|\n",
      "|  Machine Learning|        12|           12.0|    65000.0|\n",
      "|    Virual Reality|         3|            3.0|    20000.0|\n",
      "|      Data Science|         8|            8.0|    36000.0|\n",
      "|      Data Science|         5|            5.0|    29000.0|\n",
      "|  Machine Learning|        10|           10.0|    43000.0|\n",
      "|    Virual Reality|         9|            9.0|    39500.0|\n",
      "|Wearable Computing|         5|            5.0|    26000.0|\n",
      "|      Data Science|         7|            7.0|    30000.0|\n",
      "|    Virual Reality|         5|            5.0|    30000.0|\n",
      "|Wearable Computing|         3|            3.0|    27000.0|\n",
      "|  Machine Learning|         8|            8.0|    35000.0|\n",
      "+------------------+----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(['Major', 'Experience']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90ca529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Experience=3, avg(Salary)=23500.0),\n",
       " Row(Experience=4, avg(Salary)=23625.0),\n",
       " Row(Experience=5, avg(Salary)=28333.333333333332),\n",
       " Row(Experience=6, avg(Salary)=27000.0),\n",
       " Row(Experience=7, avg(Salary)=30666.666666666668),\n",
       " Row(Experience=8, avg(Salary)=35500.0),\n",
       " Row(Experience=9, avg(Salary)=39500.0),\n",
       " Row(Experience=10, avg(Salary)=49000.0),\n",
       " Row(Experience=12, avg(Salary)=65000.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Average Salary which groupby 'Experience'\n",
    "sorted(df_pyspark.groupBy('Experience').agg({'Salary': 'mean'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23ae78b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Salary=20000, count=1),\n",
       " Row(Salary=23000, count=1),\n",
       " Row(Salary=23500, count=1),\n",
       " Row(Salary=24000, count=2),\n",
       " Row(Salary=26000, count=1),\n",
       " Row(Salary=27000, count=2),\n",
       " Row(Salary=29000, count=1),\n",
       " Row(Salary=30000, count=3),\n",
       " Row(Salary=32000, count=1),\n",
       " Row(Salary=35000, count=1),\n",
       " Row(Salary=36000, count=1),\n",
       " Row(Salary=39500, count=1),\n",
       " Row(Salary=43000, count=1),\n",
       " Row(Salary=46000, count=1),\n",
       " Row(Salary=55000, count=1),\n",
       " Row(Salary=58000, count=1),\n",
       " Row(Salary=75000, count=1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_pyspark.groupBy(df_pyspark.Salary).count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9f2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
